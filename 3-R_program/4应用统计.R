#前面章节涵盖了 R 语言的基本编程语法，本章将围绕应用统计、数据清洗、文档沟通方面
#展开。
#R 语言就是因统计分析而生的编程语言，可以很方便地完成各种统计计算、统计模拟、统
#计建模等。 统计学是关于数据的科学，是一套有关数据收集整理(获取及预处理数据)、
#描述统计(汇总、图表描述)、分析推断(选择适当的统计方法研究数据，并从数据中提取
#有用信息进而得出结论)的方法。
#先来理清几个概念。
#(1)随机变量 
#当一件事情的结果无法预料时，就属于随机现象。表示随机现象的一组结果的变量就是随
#机变量。
#比如，调查了 100 个人的身高，这 100 个身高的数据是身高这一随机变量的数据。并不是
#说这些身高值是不固定可变的，而是这 100 个身高值是一次调查的结果，再调查 100 个人
#就是另一组不同的 100 个身高值。
#(2)概率分布 
#随机变量既然是随机的，还有必要研究它吗?有必要!因为把多个随机结果放在一起的时候，
#就能发现一定的规律性。比如 100 个人的身高可能对称地分布在 175cm 附近，和 175cm
#相差越多，人数越少，即表现出一种正态分布规律性。
#随机现象五花八门，但每一种随机现象表现出来的规律性是固定的，用数学语言表达出来
#就是概率分布。不同的概率分布就是不同随机现象规律性的数学描述。
#同一种概率分布，也不尽相同，这是由不同参数值决定和区分的。
#比如对于正态分布 N(μ， σ2)，μ 和 σ 就是参数，它们只要取不同的值，就会获得不同的
#分布形状，例如:
library(tidyverse)
tibble(
  x = seq(-4,4,length.out = 100),
  `μ=0, σ=0.5` = dnorm(x, 0, 0.5),
  `μ=0, σ=1` = dnorm(x, 0, 1),
  `μ=0, σ=2` = dnorm(x, 0, 2),
  `μ=-2, σ=1` = dnorm(x, -2, 1)
) %>%
  pivot_longer(-x, names_to = "参数", values_to = "p(x)") %>% 
  ggplot(aes(x, `p(x)`, color = 参数)) +
  geom_line()

#当 μ 和 σ 取不同的数值时，所得到的结果如图 4.1 所示。 统计学常用到四大概率分布，
#即正态分布、t 分布、卡方分布、F 分布。

#(3)概率论与数理统计 
#概率论就是研究随机现象的规律性，即各种概率分布及性质的理论。因为数理统计所研究
#的数据是带有随机性的，所以就需要借助概率论中的概率分布理论加以描述和做出统计推断
#因此，人们常说，概率论是数理统计的理论基础，数理统计是概率论的一种应用

#(4)区分数据类型
# 常见的数据类型可按图 
# 定量数据：连续型数据、离散型数据
# 定性数据：名义型数据、有序型数据
# 区分数据类型非常有必要，因为不同的数据类型适用的统计分析方法是不同的!

#(5)总体和样本
#总体(population):包含所研究的全部个体(数据)的集合
#样本(sample):从总体中抽取的一部分个体的集合，样本所包含的个体数目称为样本量
#抽样的目的是根据样本数据提供的信息推断总体的特征，或者说是用样本统计量推断总体参数。
#例如，要研究哈尔滨市成年男性的身高，则所有哈尔滨市成年男性的身高数据就是总体， 
#但实际上不可能把所有人的身高都测量一遍，只能是随机抽取一部分(比如 100 人)，
#测得 100 人的身高数据就是样本，样本量是 100。
#抽样调查结果的可靠性不在于样本数量大不大(当然也不能太少)，更主要的是科学抽样， 
#使样本足够代表总体。
#身高数据大致服从正态分布，所有哈尔滨市成年男性身高的均值 μ 和标准差 σ，就是总体
#参数。用样本的 100 人的平均身高作为 μ 的估计值，就是用样本统计量来推断总体参数。

#(6)参数与统计量
#参数(parameter):用来描述总体特征的概括性值，是研究者想要了解的总体的某种特征值，
#如总体均值(μ)、总体方差(σ2)、总体比例(π)等。
#统计量(statistic):用来描述样本特征的概括性数字度量，是根据样本数据计算出来的
#量。由于抽样是随机的，因此统计量是样本的函数。与上面总体参数对应的统计量是样
#本均值(x)、样本方差(s2)、样本比例(p)等。 由于总体数据通常是未知的，故参数是未
#知常数。因此才要进行抽样，根据样本计算出相应的统计量值来估计总体参数值。
#本章主要使用 rstatix 包、tidymodels::infer 包实现整洁的应用统计，此外 easystats 
#生态系列也值得关注。


#4.1 描述性统计 
#描述性统计主要是通过计算汇总统计量、绘制统计图来描述数据。
#4.1.1 统计量
#本节讨论的统计量是指样本统计量。
#1.数据位置的统计量 
#(1)均值(Mean)
#均值用于度量数据分布的中心位置，计算公式如下:
#(2)中位数(Median) 
#中位数是位于最中间的那个数据，比中位数大和小的数据各占观测值的一半。先将数据从
#小到大排序为:x(1),...,x(n)，然后计算中位数，计算公式如下:
#中位数的优点是具有稳健性，即不受个别极端数据的影响。一般来说，正态分布的数据用
#均值描述，偏态分布的数据最好是用中位数描述。比如，人均工资是经过平均计算获得的，
#中位数工资才是更合适的中间收入。
#(3)分位数(Quantile)
#中位数是 0.5 分位数，即位于 0.5 位置的数。
#0.25 分位数，称为下四分位数(Q1)，是位于 0.25 那个位置的数，即比它小的数占比是0.25,
#比它大的数占比是 0.75。同理，0.75 分位数，称为上四分位数(Q3)。
#更一般地，p 分位数，是位于 p 位置的数，即比它小的数占比是 p，比它大的数占比是 1−p
#或者说有 np 的数比它小，有 n(1−p)的数比它大。 
#(4)众数(Mode)
#众数是观测值中出现次数最多的数，对应分布的最高峰。众数常用于分类数据，即出现频数最高的值。
#计算数据位置的统计量，可使用以下函数
# mean(x):计算数值向量 x 的均值
# median(x):计算数值向量 x 的中位数
# quantile(x, p):计算数值向量 x 的 p 分位数
# rstatix::get_mode(x):计算向量 x 的众数
#2.数据分散程度的统计量 
#(1)极差(Range)
#极差就是数据中的最大值和最小值之差。
#(2)四分位距(Interquartile range) 
#四分位距是上下四分位数之差，计算公式如下:
#(3)样本方差(Variance)
#注意，分母除的是 n−1，这是为了保证用样本方差估计总体方差时，得到的是无偏估计。
#这个 n−1 也是自由度，在统计学中，几乎所有方法、所有统计量都会涉及自由度。
#自由度是计算样本统计量时能够自由取值的数值的个数。
#对于总体方差公式(除以 n)，是 n 个样本自由地从总体里抽取。但是样本方差公式时多了
#一个约束条件，它们的和除以 n 必须等于样本均值 x，所以自由度 n 减去 1 个约束条件
#对自由度的损失，等于 n−1。
#不同统计方法的自由度不一样，但基本原则是每估计 1 个参数，就需要消耗 1 个自由度。 
#以回归分析为例，若有 m 个自变量，则需要估计 m+1 个参数(包含截距项)。那么模型的 F 
#检验用到的自由度是 n−(m+1)，这意味着只剩下 n−(m+1)个可以自由取值的数值用来估计模
#型误差。
#(4)样本标准差(Standard Deviation) 
#样本方差的平方根即为标准差 s，标准差的量纲与原数据一致。
#(5)变异系数(Coefficient of Variation) 
#变异系数是标准差占均值的百分比，可用于比较不同量纲数据的分散性，其计算公式如下:
#对于该统计量的 R 实现如下。
#max(x)-min(x):计算数值向量 x 的极差。
#IQR(x):计算数值向量 x 的四分位距。
#var(x):计算数值向量 x 的样本方差。
#sd(x):计算数值向量 x 的样本标准差。
#100*sd(x)/mean(x):计算数值向量 x 的变异系数。
#3.数据分布形状的统计量
#(1)偏度(Skewness)
#偏度是用于刻画数据是否对称的指标，其计算公式如下:
#偏度有三种类型:
#均值对称的数据不偏，其偏度为 0;
#右拖尾的数据是右偏，其偏度为正;
#左拖尾的数据是左 偏，其偏度为负
#(2)峰度(Kurtosis) 
#峰度是用于刻画数据是否尖峰的指标，其计算公式如下:
#峰度以标准正态分布为基准，标准正态分布的峰度为 0;
#尖峰薄尾的分布峰度为正;
#平峰厚尾的分布峰度为负
#datawizard 包提供了 skewness()和 kurtosis()函数分别用于计算偏度和峰度。
#实际上很多包提供了同时对多个变量进行(分组)描述汇总的函数，支持所有统计量。
#其中 tidy 风格的是 rstatix::get_summary_stats()和 dlookr::describe()。
#我们以前者为例稍作演示:
library(rstatix)
iris %>%
  group_by(Species) %>%
  get_summary_stats(type = "full")

#描述统计是从不同方面对数据做了概要，想要进一步了解和探索数据，离不开绘制统计图
#不同类型的数据，适用不同类型的统计图
#1.分类数据的统计图
#(1)条形图(Bar)
#条形图是比较常用的类别比较图，是用竖直(或水平)的条形展示分类变量的分布(频数)， 
#条形的高度代表频数。对原始数据绘制条形图用 geom_bar()函数;
#对汇总频数/频率的数据用 geom_col()函数绘制条形图。
#简单条形图和堆叠条形图可参考 3.1 节，这里看一个稍微复杂一些的示例。
#以 starwars 数据集 skin_color 绘制条形图为例，做了以下几件事。
#用 fct_lump()将频数小于或等于 5 的类别做了合并。
#分组汇总，并计算各组的频数和频率。
#绘制条形图，将分类变量 skin_color 按频率做了因子重排序，实现了对“条形”排序。
#在条形旁边增加文字注释，标记该条形所占百分比。
#翻转坐标轴，变成水平条形图。 
#上述操作对应的只实现如下所示。 
df = starwars %>%
    mutate(skin_color = fct_lump(skin_color, n = 5)) %>%
      count(skin_color, sort = TRUE) %>%
      mutate(p = n / sum(n))
df

ggplot(df, aes(fct_reorder(skin_color, p), p)) +
  geom_col(fill = "steelblue") + 
    # 同 geom_bar(stat = "identity") 
    scale_y_continuous(labels = scales::percent) +
    labs(x = "皮肤颜色", y = "占比") +
    geom_text(aes(y = p + 0.04, label = str_c(round(p*100,1), "%")),
              size = 5, color = "red") +
    coord_flip()
#(2)饼图(Pie) 
#饼图是用每个扇形的圆心角大小表示每部分所占的比例，注意饼图很难去精确比较不同部
#分的大小。
#Hadley 认为饼图可以通过极坐标变换得到，因此没有提供绘制饼图的几何对象，另外从展
#示分类数据的角度来说，饼图也不是一个好的选择。
#注意:3.2 节提供了绘制饼图模板。
#(3)克利夫兰点图(Cleveland Dot Plot)
#克利夫兰点图适合展示多类别之间的比较:x 轴是分类变量，每一类对应一个类均值或频数
#(y 值)，画一个圆点;并根据 y 值大小对 x 轴类别排序;通常需要再做一次坐标翻转。
#以美国 2000 年以来的失业率数据为例，绘制克利夫兰点图，并添加横线(也可不加)
economics %>%
  group_by(year = lubridate::year(date)) %>%
  summarise(uempmed = mean(uempmed)) %>%
  filter(year >= 2000) %>%
  ggplot(aes(reorder(year, uempmed), uempmed)) +
  geom_point(size = 4, shape = 21,
             fill = "steelblue", color = "black") +
  geom_segment(aes(xend = ..x.., yend = 5)) +
  xlab("year") +
  coord_flip()

#2.连续数据的统计图 
#(1)直方图
#连续数据常用直方图来展示变量取值的分布，利用直方图可以估计总体的概率密度。
#将变量的取值范围分成若干区间。直方图是用面积而不是用高度来表示数，总面积是 100%. 
#每个区间矩形的面积恰是落在该区间内的百分数(频率)，所以矩形的高 = 频率/区间长度 
#= 概率密度 
#特别地，若区间是等长的，则矩形的高就是频率。注意:直方图矩形之间是没有间隔的。
#用 geom_histogram()函数绘制直方图。频率直方图与概率密度曲线正好搭配，因为频率
#直方图的条形宽度趋于 0，就相当于概率密度曲线。
#若想绘制频数直方图和概率密度曲线，就需要对密度做一个放大，即条形宽度*样本数倍。 
set.seed(123)
df = tibble(heights = rnorm(10000, 170, 2.5))
ggplot(df, aes(x = heights)) +
  geom_histogram(fill = "steelblue", color = "black", binwidth = 0.5) +
  stat_function(fun = ~ dnorm(.x, mean = 170, sd = 2.5) * 0.5 * 10000,
                color = "red")
#若想在同一张图上叠加多个直方图，以对比分类变量不同水平的概率分布，更适合用 
#geom_freqpoly()函数绘制频率多边形图;函数 geom_density()可以绘制核密度估计曲线

#(2)箱线图 
#箱线图是在一条数轴上完成以下操作:
#以数据的上下四分位数(Q1 和 Q3)为界画一个矩形盒子(中间 50%的数据落在盒内); 
#在数据的中位数位置画一条线段作为中位线;
#默认延长线为盒长的 1.5 倍，延长线之外的点是异常值
#箱线图的主要应用就是剔除数据的异常值，判断数据的偏态和尾重，可视化组间差异。
#用 geom_boxplot()函数绘制箱线图，例如比较不同 drv 下 hwy 的组间差异，代码如下:
ggplot(mpg, aes(x = drv, y = hwy)) +
  geom_boxplot()

#若要将箱线图水平放置，只需再加上图层 coord_flip()
#再来看一个综合的均值线与误差棒图
#以 ToothGrowth 数据集为例，先自定义分组汇总 函数计算分组均值和标准误，代码如下:
my_summary = function(data, .summary_var, ...) {
  summary_var = enquo(.summary_var)
  data %>%
    group_by(...) %>%
    summarise(mean = mean(!!summary_var, na.rm = TRUE),
              sd = sd(!!summary_var, na.rm = TRUE)) %>%
    mutate(se = sd / sqrt(n()))
}
df = my_summary(ToothGrowth, len, supp, dose)
df

pd = position_dodge(0.1)

ggplot(df, aes(dose, mean, color = supp, group = supp)) +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se),
                color = "black", width = 0.1, position = pd) +
  geom_line(position = pd) +
  geom_point(position = pd, size = 3, shape = 21, fill = "white") + 
  xlab("剂量 (mg)") + ylab("牙齿生长") +
  scale_color_hue(name = "喂养类型", breaks = c("OJ", "VC"),
                  labels = c("橘子汁", "维生素C"), l = 40) + 
  scale_y_continuous(breaks = 0:20 * 5)


#4.1.3 列联表
#对分类变量做描述统计，通常是计算各水平值出现的频数和占比，得到列联表(交叉表)。 
#以上操作可以用 table()函数实现，但功能很弱，也不够简洁。
#janitor 包提供了更强大的 tabyl()函数，可以生成一个、两个、三个变量的列联表，
#再结合 adorn_*()函数，可以很方便地按想要的格式添加行列的合计和占比等。 
#为一维列联表添加合计行，代码如下:
library(janitor)

mpg %>%
  tabyl(drv) %>%
  adorn_totals("row") %>% # 添加合计行 
  adorn_pct_formatting() # 设置百分比格式

#为二维列联表添加列占比和频数，代码如下: 
mpg %>%
  tabyl(drv, cyl) %>%
  adorn_percentages("col") %>% # 添加列占比
  adorn_pct_formatting(digits = 2) %>% # 设置百分比格式
  adorn_ns()  # 添加频数
#三维列联表是针对 3 个分类变量，结果就像多维数组的“分页”，在此不作详述。
#另外，还有很多包能将描述性统计、回归模型的结果变成规范的表格样式，比较有代表性
#的是 modelsummary 包。实验设计(表)在科研、生产中应用广泛，各种常用的实验设计可
#以用 edibble 包实现。


#4.2 参数估计 
#与总体有关的指标是参数;与样本有关的指标是统计量。统计推断的重要内容之一就是参
#数估计，即在抽样及抽样分布的基础上，根据样本统计量推断所关心的总体参数。
#4.2.1 点估计与区间估计
#参数估计主要有两种:点估计(准确，但不一定可靠)和区间估计(更可靠，但不太精确)。
#点估计就是样本统计量。比如估计哈尔滨成年男性的平均身高，样本均值 175cm 就是点估
#计;有一定概率落在 172~178cm 之间，这就属于区间估计。
#区间估计通常是指估计其 95%置信区间，即有 95%的把握认为该区间包含了总体参数，
#换句话说，如果抽样 100 次，该区间将有 95 次包含了总体参数。
#置信区间越窄反映了参数估计的精确度越高，影响它的因素一是置信水平，置信水平越高，
#置信宽度越大;二是样本量，样本量越大置信宽度越小。
#1.用标准误计算置信区间 
#即使一个代表性非常好的样本，也是无法真正等同于总体的，总会存在一定的抽样误差。 
#什么是抽样误差?比如用 100 人的平均身高作为总体参数 μ 的估计，如果再随机抽样 100
#人，又得到另一个平均身高;再随机抽样 100 人，又得到一个平均身高......做 10 次抽样，
#就可 以计算出样本统计量，即 10 个平均身高和 10 个标准差。这 10 个平均身高也可以
#计算标准差， 这就是标准误(样本统计量的标准差)，它反映了样本统计量之间差别(抽样误差)
#的大小。
#然而，实际上不可能多次抽样计算每个样本的统计量，再计算各个统计量之间的差异，而
#应该获取一个尽可能大的样本来计算标准误，具体方法是借助统计学家得到的计算公式:
#其中，s 为样本标准差，n 为样本量。可见样本量越大，标准误越小。
#标准误几乎在所有统计方法中都会出现，因为标准误的大小直接反映了抽样是否有足够的
#代表性，进而结果是否有足够的可靠性(可信度)。
#由于抽样误差的存在，如果用样本统计量直接估计总体参数，肯定会有一定的偏差。
#所以在估计总体参数时需要考虑到这种偏差大小，即用置信区间(参数估计值±估计误差)来
#估计总体参数。
#根据中心极限定理，从任何分布中抽样，只要样本量足够大，其统计量最终会服从正态分布。
#因此，估计误差通常用对应一定正态分位数的 Z 值再乘以表示抽样误差的标准误来表示。 
#例如，95%置信区间一般表示为参数估计值 ± 1.96 × 标准误。
#不同样本统计量的标准差的计算过程不同，其标准误也不同。
#均值的置信区间计算方式如下:
#若样本量较小，建议用相应 t 值代替 Z 值。
#由于比例 p的标准差为 p(1 p) ，故比例的置信区间为:
#上述传统方法依赖于中心极限定理，要求大样本近似正态分布，统计量有计算公式。
#对于某些抽样分布未知或难以计算的统计量，想要根据一个样本研究抽样样本变化带来的变异，
#就需要 Bootstrap(自助)重抽样法。
#手工实现 Bootstrap 法极其麻烦，但特别适合用计算机实现，已广泛用于统计推断
#(点估计、置信区间、假设检验)、回归模型诊断以及机器学习等。
#Bootstrap 法的基本思想是:样本是从总体中随机抽取的，则样本包含总体的全部信息， 
#那么不妨就把该样本视为“总体”，进行多次有放回抽样生成一系列经验样本，再对每个经验
#样本计算统计量，就可以得到统计量的分布，进而用于统计推断。
#可以证明，在初始样本量足够大且是从总体中随机抽取的情况下，bootstrap 抽样能够无
#偏接近总体的分布。
#以 Bootstrap 法估计统计量的置信区间，其基本步骤如下: 
#从原始样本中有放回地随机抽取 n 个构成子样本;
#对子样本计算想要的统计量;
#重复前两步 K 次，得到 K 个统计量的估计值;
#根据 K 个估计值获得统计量的分布，并计算置信区间。
#来自 tidymodels 系列的 infer 包提供了统一的、简洁的统计推断工作流(如图 4.9 所示)，
#1 在计算具体的标准误时，真正需要的可能是某些真实值或来自总体的值。
#若无法得到上述值，通常是用它们所对应 的样本估计值来代替，某些估计值
#要保证其能作为代替，可能离不开一些模型假定(理论保证)。

#注意:对于以上两式，若已知容许误差，令第 2 项小于容许误差并反解出 n，就得到了确定的样本量。

#2.Bootstrap 法估计置信区间
#涉及的主要函数如下所示。
# specify():设定感兴趣的变量或变量关系
# hypothesize():设定零假设
# generate():基于零假设生成数据
# calculate():根据上述数据，计算统计量的分布
# visualize():可视化
#另外，infer 包还提供获取、绘制 p 值和置信区间的函数
#用 infer 包实现 Bootstrap 置信区间的一般流程
#下面看一个例子，假设从某学校随机抽样了 20 名学生的身高，想要估计该学校所有学生的身高。 
#计算基于标准误的置信区间
df = tibble(
  height = c(167,155,166,161,168,163,179,164,178,156,
             161,163,168,163,163,169,162,174,172,172))
mu = mean(df$height) # 点估计: 样本均值 
mu
se = sd(df$height) / sqrt(nrow(df)) # 标准误
mu + c(-1,1) * qnorm(1-0.05/2) * se # 基于标准误的置信区间

#计算基于 Bootstrap 法的置信区间
library(infer)
boot_means = df %>%
  specify(response = height) %>%
  generate(reps = 1000, type = "bootstrap") %>% # 1000 次 bootstrap 
  calculate(stat = "mean") # 计算统计量: 样本均值
boot_means


boot_ci = boot_means %>%
  get_ci(level = 0.95, type = "percentile") # bootstrap置信区间
boot_ci

visualize(boot_means) +
  shade_ci(endpoints = boot_ci)
#这里只是阐述 bootstrap 法估计统计量和置信区间的基本用法，更多案例可参阅 infer
#包 Vignettes。

#4.2.2 最小二乘估计
#最小二乘估计(Ordinary Least Squares，OLS)常用于估计线性回归、曲线拟合的参数，其
#思想是让实际值与模型预测值的总偏离达到最小，从而得到最优的模型参数估计值。
#下面用一元线性回归来阐述。
#设有 n 组样本点(xi, yi)其中 i = 1, ..., n，比如有 10 组(n = 10) 
#广告费用与销售额的数据，那么绘制散点图的代码如下:
sales = tibble(
  cost = c(30,40,40,50,60,70,70,70,80,90),
  sale = c(143.5,192.2,204.7,266,318.2,457,333.8,312.1,386.4,503.9))
ggplot(sales, aes(cost, sale)) +
  geom_point()
#可见，这些散点大致在一条直线上，一元线性回归就是寻找一条直线，使得直线
#与这些散点拟合程度最好(即散点越接近直线越好)
#比如画这样一条直线，一元线性回归模型方程可写为:
#   y0 1x
# 其中， 0 和 1 为待定参数，目标是找到样本点最接近的直线对应的 0 和 1 。那么，怎么
# 刻画这种“最接近”?
#   yˆi  0  1xi 是与横轴 xi 对应的直线上的点的纵坐标(模型预测值)，它与样本点 xi 对应的
# 真实值 yi 之差，就是预测误差(虚线长度):
#    i  y i  yˆ i , i  1 ,  , n
# 以上公式适合描述散点到直线的“接近程度”，但描述绝对值时不容易计算，改用以下公式:
#   2(yyˆ)2, i1,,n iii
# 取到:
#   我们需要让所有散点总体上最接近该直线，故需要让总的预测误差 J 最小，J 的计算公式如下: n2n 2
# J0,1 (yiyˆi)  [yi01xi] i1 i1
# 于是问题转化为优化问题:
#   n 2 argminJ0,1 [yi 0 1xi]
# 0,1 i1
# 其中，argmin 意思是求右式的值达到最小时所对应的参数 0 和 1 ，这就是“最小二乘法”，
# 有着很直观的几何解释。 这是个求二元函数极小值问题。根据微积分知识，二元函数极值是在一阶偏导等于 0 点处
# J n
# 
# 2 [yx]0
# 0 i1  J n
# i01i
#  2[y  x]x 0
# i01ii
# 解关于 0 和 1 的二元一次方程组，可得:
#     0  y   1 x
# n n n
#  xy y x
# (x x)y y
# 
#  x  n x
# ii
# 1 n x2xn x n (xx)2
# i
# i1  i1
# ii  i1i i1i i1i
#  i1
# 其中:
#   1n 
# i  i1
# 1n
#  y  n y  i1
# 更一般地，用最小二乘法估计多元线性回归、非线性回归(拟合)的待定参数，也是类似 的，
#只需要将线性预测值改成模型预测值即可，具体公式如下:
#   n
# argminJβ [y fx,β]2
# β

#线性回归的最小二乘法估计可用 lm()函数实现，非线性回归的最小二乘法估计可用 nls() 函数实现。
#现有我国 2003-2019 年历年电影票房数据，针对这些数据绘制散点图，代码如下:

df = readxl::read_xlsx("data/历年累计票房.xlsx") %>% 
  mutate(年份 = 年份 - 2002)
p = ggplot(df, aes(年份, 累计票房)) + 
  geom_point(color = "red", size = 1.5) + 
  labs(x = "年份(第几年)", y = "累计票房(亿元)")
p

#我们想用 nls()做非线性拟合，就要寻找最优的参数值1 、2 、3 
#非线性拟合的算法非常依赖于参数初始值的选取，如果参数选取适当(离估计值不远)，
#很快就能收敛到最优估计，否则迭代很可能无法收敛。
#参数1 对应人口容纳量上限，大致为曲线的拐点值(目测约为 400)的 2 倍，一旦确定了1 ， 
#则有如下公式:
# logit( p)  ln 其中，
# 的估计值，代码如下:
# p
# 1 p 称为
# Logit
# 1 变换。于是，我们用
# lm()
#  
# 做线性回归即可得到 2 和 3
lm.fit = lm(car::logit(累计票房 / 800) ~ 年份, df) 
coef(lm.fit)

#这样就得到了一组较好的参数初始值:1  800，2  5.14，3  0.39 。
#接着就可以用 nls()做非线性拟合，这里需要提供模型公式和初始参数值，代码如下:
log.fit = nls(累计票房 ~ phi1 / (1 + exp(-(phi2 + phi3 * 年份))), 
              data = df,
              start = list(phi1 = 800, phi2 = -5.14, phi3 = 0.39))
coefs = coef(log.fit)
coefs

#根据所得模型，绘图看一下拟合效果，同时这也是已知函数表达式，绘制 ggplot 图形的
#方法，代码如下:
LogFit = function(x) coefs[1] / (1 + exp(-(coefs[2] + coefs[3] * x)))
p + geom_function(fun = LogFit, color = "steelblue", size = 1.2)
#注意:nls()拟合依赖于初始值和 selfstart 设置，容易拟合失败，若拟合失败可以用 gslnls 包


#4.2.3 最大似然估计 
#先来介绍一下频率学派与贝叶斯学派，频率学派和贝叶斯学派对世界的认知有本质上的不
#同，具体如下所示
#频率学派认为世界是确定的，有一个本体，这个本体的真值是不变的，我们的目标是要
#找到这个真值或真值所在的范围
#而贝叶斯学派认为世界是不确定的，人们对世界先有一个预判，而后通过观测数据对这
#个预判做调整，我们的目标是要找到描述这个世界的最优的概率分布
#在对数据建模时，用 θ 表示模型的参数，那么解决问题的本质就是求 θ
#频率学派认为:存在唯一的真值 θ
#比如抛一枚硬币 100 次，有 20 次正面朝上，要估计抛硬币正面朝上的概率，即伯努利分
#布的参数 p = P(正)。
#在频率学派看来， p  20 / 100  0.2 ，简单直观。当抛硬币次数趋于无穷时，
#该方法能给出精确的估计;然而当次数不够多时，可能会产生严重的偏差。
#贝叶斯学派认为: θ 是一个随机变量，符合一定的概率分布。 在贝叶斯学派里有两大输入
#和一大输出，输入是先验(prior)和似然(likelihood)，输出是后验(posterior)。先验，
#即P(θ)，是指在没有观测到任何数据时对θ的预先判断。比如抛一枚硬币，一种可行的先验
#是认为该硬币有较大概率是均匀的;似然，即P(X |θ)，是假设θ已知后观察到的数据应该是
#什么样子的;后验，即 P(θ | X ) 是最终的参数分布。贝叶斯估计的基础是贝叶斯公式，如下所示:
#P(θ|X) P(X |θ)Pθ PX
#同样是抛硬币，对一枚均匀硬币抛 5 次得到 5 次正面，如果先验认为该硬币大概率是均匀的，
#那么参数 p ，即 P(θ | X ) 是一个概率分布，最大值会介于 0.5~1 之间，而不是武断地认为 p  1
#随着数据量的增加，参数分布会越来越向基于数据估计的参数靠拢，先验的影响力会越来越小;
#如果先验是均匀分布(本质上表示对事物没有任何预判)，则贝叶斯方法等价于频率方法。
#贝叶斯统计也越来越兴起，特别是出现了专门用于贝叶斯推断的 Stan 语言，其与 R 语言
#的接口是 rstan 包以及更方便的统计建模包 rstanarm、brms 和 tidybayes 等。
#最大似然估计(Maximum Likelihood Estimation)是频率学派常用的方法，其思想是既然抽
#取到现在的样本数据，那么最优的模型参数应选择让这些样本数据最有可能出现的参数值。
#比如，你和猎人同时开枪，结果是猎物被击中。用最大似然估计来解释即猎物是猎人打中的，
#而不是你打中的!因为猎人打中的概率比你大。
#假设数据 x1, x2, ..., xn 是独立同分布的一组抽样，记为 x = (x1, x2, ..., xn)，
#则最大似然法估计参数 θ ，可推导如下:
#详情请参阅知乎网站上夏飞发布的“聊一聊机器学习的 MLE 和 MAP:
#最大似然估计和最大后验估计”一文
#在上述推导过程中，第 1 行到第 2 行是由于独立同分布;
#第 2 行到第 3 行是由于 ln() 单调
#递增，故做对数变换不影响求最大参数值。最后要优化的函数记为
# 以上函数称为对数似然函数，其中，P(xi |θ)为给定的θ下出现xi 的概率(对于离散情形是
# 概率，对于连续情形是概率密度)。 于是，最大似然估计的一般步骤是先推导出对数似然函数，
# 再做最大化寻优即可。
# 后一步可用自带的 optimize()函数或者 maxLik 包中的 maxLik()函数来实现。 
# 离散情形，估计伯努利分布参数
# 例如已发生事件是抛 10 次硬币，出现 3 次正面，用最大似然法估计参数 p = P(正)。 
# 抛硬币服从伯努利分布，该事件发生的概率(似然函数)可表示为:
# 从而，对数似然函数为:
# P(x|p)C3 p3(1p)7 10
# (p|x)lnC3 3lnp7ln(1p) 10
# (θ|X)lnP(x |θ) i
#注意，第一项是常数，不妨忽略掉它，不影响优化目标。对于上述公式，手动计算也很容易，
#但如果要用 maxLik 包实现，就要先定义对数似然函数:
#loglik = function(p) 3 * log(p) + 7 * log(1-p)
#再调用 maxLik()函数，此时需传递对数似然函数，并提供迭代初始值: 
library(maxLik)
stdEr(m) # 估计的标准误
#不出所料，最优估计 pˆ  0.3 ，也就是正面出现的频率! 
#连续情形，估计正态分布参数离散情形，考虑的是单个点的概率。
#而对于连续情形，单个点的概率为 0 ，此时考虑包含点的任意小区间段的概率才有意义，
#也就是概率密度:
m = maxLik(loglik, start = 0.5) coef(m) # 最优参数估计值

#下面以 mtcars$mpg 数据(n=32)为例，用最大似然法估计正态分布的参数 μ 和 2 ，则
#该数据出现的概率(似然函数)为:
#从而，对数似然函数为:
# 1n 2 (,|x)2ln(2π)nln22 (xi )
#1 同样忽略掉第一项常数，定义对数似然函数，再调用 maxLik()寻优，代码如下:
# loglik = function(theta) {
#   stdEr(m) # 估计的标准误 ## mu sigma
#   通过上述模型绘图，代码如下:
#     ggplot(mtcars, aes(mpg)) +
#     可视化的结果如图 4.15 所示。
#   图 4.15 实际上，正态分布两个参数的最大似然估计分别为:
#     mu = theta[1]
#   sigma = theta[2]
#   n = nrow(mtcars)
#   - n*log(sigma) - 1 / (2*sigma^2) * sum((mtcars$mpg - mu)^2)
# }
m = maxLik(loglik, start=c(mu=30, sigma=10)) coef(m) # 最优参数估计值
geom_histogram(binwidth = 1, fill = "steelblue") +
  stat_function(fun = ~ dnorm(.x, mean = 20.09, sd = 5.93) * 32,
                color = "red", size = 1.2)
#可视化该最大似然估计效果
# ˆ  x 21n 2
# ˆn (xix) i1
#  线性回归系数的最大似然估计
# 在真实数据中，一组 x 值对应的 y 观测值可以看作来自真实 y 值的一次抽样，因为 y 值可能
# 受多种因素的影响，故可以假设任意一组 x 值对应的真实 y 值是服从正态分布的随机变量。
# 根据最大似然估计的思想，最优的回归系数就是让 y 观测值出现的概率最大时所对应的回归系数，
# 公式如下:
#   其中，i 为预测误差，即不能被线性模型刻画的部分。根据线性回归模型假设，
#   i 独立同分布于 N (0, 2 ) ，否则说明数据不适合用线性回归模型建模。
#   yi 0 1xi i,i1,,n

#根据正态分布的性质yi N0 1xi,2，从而在xi和已知的条件下，yi的概率密度计
# f(yi |xi,)
# 从而，所有 y 的观测数据出现的概率(似然函数)为:
#   1  [yi01xi]2
# exp , i1,,n
# 2π 22  
# n
# ii
# f(y|x,)
#   [yx]
# i1
# f(y |x,)
# n1i01i2
#   exp 
# 2 2π 2 
# 于是，对数似然函数为:
#   i1    (0 , 1 | x)  ln( f ( y | x,  ))
# n2  1 [yx]
# nln  i 0 1i
# 2
# 注意:maxLik()函数在进行优化时，默认是根据数据计算数值梯度(只适合简单问题)。若推
#导出梯度(甚 至是 Hessian 矩阵)的解析式，并提供给相应参数，则估计速度更快，结果更稳定。
#  2π i1 2
# 注意，要做的是选取适当的 0 和 1 让上式达到最小值，公式的第一项以及第二项中的 2 2
# 不起作用。故最大化该对数似然函数，就等价于:
#   n 2 argmin [yi 0 1xi]
# 0,1 i1 这与最小二乘估计是等价的!
#   最大后验估计(MAP)
# 最大后验估计是贝叶斯学派常用的估计方法，同样假设数据 x1, x2, ..., xn 是独立同
#分布的一组抽样，记 x = (x1, x2, ..., xn)，则最大后验估计参数 θ 的推导过程基于贝叶斯公式可得:
#   ˆ
# θMAP  argmaxP(θ | x)
#  argmaxlnP(θ | x)
#  argmaxlnP(x | θ)  lnP(θ)  lnP(x)  argmaxlnP(x | θ)  lnP(θ)
# 可见，与最大似然估计的不同之处在于相差一个先验 ln(P(θ)) 。有趣的是，若该先验是正态分布，
#则 MAP 等价于 MLE+L2 正则。



#4.3 假设检验 
#4.3.1 假设检验原理
#实际上，我们只能得到所抽取样本(部分)的统计结果，又想进一步推断总体(全部)的特征。
#但是这种推断必然有可能“犯错”，那“犯错”的概率为多少时，我们能接受这种推断呢? 为此，
#统计学家基于小概率反证法思想开发了假设检验这一统计方法进行统计检验。假设
#检验的基本逻辑是，如果原假设是真的，则检验统计量(样本数据的函数)将服从某概率分布，
#具体如下。
#先提出原假设(也称为零假设)，接着在原假设为真的前提下，基于样本数据计算出检
#验统计量值，与统计学家建立的这些统计量应服从的概率分布进行对比，就可以知道在
#百分之多少(P 值1)的机遇下会得到目前的结果。
#若经比较后发现，出现该结果的概率(P 值)很小，就是基本不会发生的小概率事件; 则可以
#有把握地说，这不是巧合，拒绝原假设是具有统计学上的意义的;否则就不能拒 绝原假设。
#关于原假设与备择假设，我们可以按以下方式理解。
#原假设( H0 ):研究者想收集证据予以反对的假设。
#备择假设( H1 ):研究者想收集证据予以支持的假设。 
#假设检验判断方法有 P 值法和临界值法。
#以 t 检验为例，双侧检验: H0 :   0，H1 :   0 。
#在原假设H0下，根据样本数据可计算出t统计量值t0。 
#P值Pt≥t0，表示t0的双侧尾部的面积
#若 P  0.05 (在双尾部分)，则在 0.05 显著水平下拒绝原假设 H 0
#临界值法以显著水平处的统计量值为界限，中间白色区域是接受域，两侧阴影部分是拒绝域，
#看统计量值 t0 落在哪个部分来下结论。
#左侧检验:H0 :≥0，H1:0
#在原假设H0下，根据样本数据计算出t统计量值t0。 
#P值Pt≤t0表示t0的左侧尾部的面积。
#若 P  0.05 (在左尾部分)，则在 0.05 显著水平下拒绝原假设 H 0 。
#右侧检验:H0 :≤0，H1:0
#在原假设H0下，根据样本数据计算出t统计量值t0。
#P值 Pt≥t0, 表示t0的右侧尾部的面积。
#若 P  0.05 (在右尾部分)，则在 0.05 显著水平下拒绝原假设 H 0 。
#1假设检验的 P 值，是在 H0 为真时根据检验统计量服从的理论概率分布计算的，
#衡量的是在原假设 H0 下出现当前观 测结果可能性的大小。
#1.假设检验的两类错误
#I型错误:在原假设 H0 为真时，仍然有可能得到检验统计量的 P 值很小，因此拒绝了 H0 。
#这就犯了I型错误，犯I型错误的概率用 表示(一般设为 0.05)。显然，犯I型错误的概率等
#于显著水平1，若要减小它，只需要减小显著水平，比如减小到 0.01
#II型错误:在备择假设为真时，但由于种种原因(抽样运气不好、样本量不够等)并没有拒绝
#原假设，这就犯了II型错误，犯II型错误的概率用  表示(一般设为 0.2)
#2.假设检验的功效
#在备择假设为真时，拒绝原假设的概率，称为假设检验的功效(Power, 等于1  )，它反
#映了你对研究结果的把握程度。
#若备择假设为真，则拒绝原假设的概率应该是 100%，故假设检验的功效越大越好，通常要
#求不低于 80%。提高假设检验功效的一种可行办法是，增大样本量。一旦设定了显著水平
#(如 0.05)和功效(如 0.8)，根据检验统计量就可以科学地计算出样本量。
#用 pwr 包可以很方便地计算常用统计检验的功效或要达到某功效需要的样本量。以右侧 
#t 检验为例，pwr 包的使用方法如下:
#注意:若不用研究就知道差异应该很大，Cohen 效应量取值应大一些，比如 0.8。 
#4.3.2 基于理论的假设检验
#基于理论的假设检验，可分为两类。
#参数检验:要求样本来自的总体分布已知，对总体参数进行估计;优点是数据信息得以
#充分利用，统计分析效率高;缺点是对数据质量要求高、适用范围有限。
#非参数检验:不依赖数据的总体分布，也不对总体参数进行推断;优点是不受总体分布限制，
#适用范围广，对数据质量要求不高;缺点是检验功效相对较低，不能充分利用数据信息。 
#选择原则:首先考察是否满足参数检验的条件，若满足，则优先选用参数检验;若不满足，
#则只能采用非参数检验。 对于定量数据和定性数据适用的假设检验方法是不同的，常用的
#rstatix包提供了一个与tidyverse设计哲学一致的简单且直观的管道友好型框架，可用于
#执行上述经典统计检验，该框架支持结合 group_by()做分组检验，且将检验结果转化为整
#洁的数据框输出
#rstatix 包提供的常用的假设检验函数及分类如下。 
#比较均值
#t_test():适用于单样本、两独立样本、配对t检验。
#wilcox_test():适用于单样本、两独立样本、配对Wilcoxon检验。
#因此，假设检验的显著水平可理解为，若原假设为真，拒绝原假设的概率。

library(pwr)
# 每组样本量50, Cohen效应量取值0.5, 显著水平取值0.05, 计算功效
pwr.t.test(n = 50, d = 0.5, sig.level = 0.05, alternative = "greater")
# Cohen效应量取值0.5, 显著水平取值0.05, 功效取值0.8, 计算每组样本量 
pwr.t.test(power = 0.8, d = 0.5, sig.level = 0.05, alternative = "greater")

#sign_test():适用于单样本、两样本符号秩检验
#anova_test():适用于独立测量、重复测量、混合方差分析
#kruskal_test():适用于Kruskal-Wallis秩和检验
#friedman_test():适用于Friedman检验

#比较比例
#prop_test():适用于单样本、两样本比例的z检验
#fisher_test():适用于Fisher精确检验，适用于单元格频数<5的情况
#chisq_test():拟合优度、同质性、独立性卡方检验
#binom_test()/multinom_test():精确二项/多项检验
#mcnemar_test()/cochran_qtest():适用于McNemar卡方检验，对比两对或多对比例有无差异
#prop_trend_test():适用于趋势卡方检验
#其他检验
#shapiro_test():检验一元正态性1
#mshapiro_test():检验多元正态性 
#levene_test():检验方差齐性
#cor_test():检验相关性
#使用一个假设检验，首先要明确其原假设和备择假设是什么;
#然后调用相应函数得到检验 结果;
#最后解读结果。根据 P 值得到结论:若 P<0.05，则拒绝原假设，否则不能拒绝原假设。

#1.方差分析
#方差分析是针对连续变量的参数检验，检验多个分组的均值有无差异，其中分组是按影响
#因素的不同水平值组合进行划分的。它是对总变异进行分解，看总变异由哪些部分组成，
#这些部分间的关系如何。
#方差分析对数据的要求:满足正态性(各组分别来自正态总体)和方差齐性(各组方差相 等)，
#在这两个条件下，若各组有差异，则只可能是来自影响因素的不同水平。
#Kolmogorov-Smirnov正态性检验可用ks.test(x, "pnorm", mean=mean(x), sd=sd(x))实现

#方差分析可用于:
#完全随机设计(单因素)、随机区组设计(双因素)、析因设计、拉丁方设计和正交设计等;
#对两因素间交互作用差异进行显著性检验;  进行方差齐性检验。
#方差分析假定每一个观测值都由若干部分累加而成，也即总的效应可分解为若干部分，
#每一部分都有特定含义，称为效应的可加性。根据效应的可加性，将总的离均差平方和分
#解成若干部分，每一部分都与某一种效应相对应。总自由度也被分成相应的各个部分，
#各部分的离均 差平方除以各自的自由度得出各部分的均方(Mean Square)，两个均方之比
#服从 F 分布。
#以焦虑症的治疗疗效为例，一个因素是治疗方案，有两种治疗方案，即该因素有两个水平
#(治疗方案称为组间因子，因为每个患者只能被分配到一个组别中，没有患者同时接受两种治疗);
#再考虑另一个因素治疗时间，也有两个水平:治疗 5 周和治疗 6 个月，同一患者在 5 周
#和 6 个月不止一次地被测量(两次)，称为重复测量(治疗时间称为组内因子，因为每个患者
#在所有水平下都进行了测量)。 
#建立方差分析模型时，既要考虑两个因素治疗方案和治疗时间(主效应)，又要考虑治疗方
#案和时间的交互影响(交互效应)，这称为两因素混合模型方差分析。 当某个因素的各个水
#平下的因变量的均值呈现统计显著性差异时，必要时可作两两水平间的比较，这称为均值间的两两比较
#以 ToothGrowth 数据集为例，包含 60 只豚鼠的牙齿生长数据，有两种喂食方法:
#OJ、VC，各喂食剂量有 3 个水平:0.5mg、1mg、2mg，这样就分配为 6 组，每组各 10 只
library(rstatix)
df = ToothGrowth %>%
  mutate(dose = factor(dose))
head(df, 3)
#牙齿长度(len)为因变量，关于喂食方法(supp)和剂量(dose)做两因素混合模型方差分析，
#其模型分解公式为:
#下面先验证做方差分析的前提条件，再做两因素方差分析:

# 正态性检验(H0:正态) 
shapiro_test(df, len)
# 检验方差齐性(H0:方差齐) 
levene_test(df, len ~ supp * dose)
# 两因素混合模型方差分析 
anova_test(df, len ~ supp * dose)
#在以上代码中，len ~ supp * dose是设定模型公式，遵从R的formula语法，~左 边是因变量，
#~右边是自变量公式，supp * dose是supp + dose + supp:dose的简写， supp:dose 表示这
#两个变量的交互项。
#可见，正态性和方差齐性均满足，方差分析结果的主效应 supp 和 dose 都非常显著
#(P 值 都远小于 0.05)，交互效应也显著(P 值 = 0.022<0.05)，表明 supp 和 dose 的
#协同变化下的各组均值显著不同。若交互作用不显著，可以只做去掉交互效应的方差分析
#若要做 Tukey’HSD 组间的两两比较(多重比较)，可参考以下代码及结果: 
tukey_hsd(df, len ~ supp * dose)

#方差分析要求观测之间相互独立，而重复测量数据是在分组因素之外，分别在组内不同的
#时间点上重复测量同一个体获得因变量的观测值，或者是通过重复测量同一个体的不同部位
#获得因变量的观测值。这就不再具有相互独立性，需要用专门的方法来处理，这个处理过程
#称为重复测量方差分析
#重复测量数据常用来分析因变量在不同时间点上的变化。分析前需要对重复测量数据之间
#是否存在相关性进行球形检验，若 P 值<0.05 则说明存在相关性，应该做重复测量方差分析
#重复测量方差分析的模型公式一般形式为:
#  Y ~ B*W ErrorSubject/W
#其中，B 为组间因子，W 为组内因子，Subject 为个体标记。
#为了方便演示，我们给 df 增加 1 列 ID 为 1:10，并重复 6 次，相当于是一共是 10 只豚鼠， 
#重复测量了 6 次牙齿长度，然后做重复测量方差分析
df %>%
  mutate(ID = rep(1:10, 6)) %>%
  anova_test(len ~ supp * dose + Error(ID / (supp * dose)))

#上述球形检验的结果表明，重复测量数据存在相关性，两个主效应都很显著，交互效应不显著
#注意:重复测量方差分析也要求满足方差齐性，若不满足，则可以考虑用 lme4::lmer()拟合
#混合效应 模型
#另外，bruceR 包整合了丰富的方差分析和结果的格式化文档输出



#2.卡方检验
#卡方检验是针对无序分类变量的非参数检验，其理论依据是实际观察频数 f0 与理论频数 fe
#(又称期望频数)之差的平方再除以理论频数所得的统计量，近似服从 χ2 分布。 
#卡方检验一般用来检验无序分类变量的实际观察频数和理论频数分布之间是否存在显著差
#异，要求如下:
#分类变量相互排斥，互不包容;
#观测相互独立;
#样本容量不宜太小，理论频数大于或等于 5，否则需要进行校正(合并单元格或校正卡方值)
#卡方检验常用于以下情况
#拟合优度检验:检验某连续变量的数据是否服从某种分布，检验某分类变量各类的出现概率
#是否等于指定概率。
#独立性/关联性检验:检验两个分类变量是否相互独立。
#同质性检验:检验两组频数是否来自同一总体，若是，则每一类出现的概率应该是差不多的;
#检验两种方法的结果是否一致，例如两种方法对同一批人进行诊断，其结果是否一致
#以检验 Titanic 船舱等级与是否生存之间是否相互独立为例，其原假设和备择假设是:
#H0 :相互独立 H1 :不相互独立 

titanic = read_rds("data/titanic.rds")
tbl = titanic %>%
  janitor::tabyl(Survived, Pclass)
tbl

#P 值几乎等于 0，因此拒绝原假设，故结论是船舱等级与是否生存之间有关联。
#若要进一步 比较各等级的船舱之间生存率是否有差异，可使用以下代码:
pairwise_prop_test(as.matrix(tbl[,-1]))

#4.3.3 基于重排的假设检验
#4.2.1 节讲到用 infer 包提供的工作流实现 Bootstrap 法估计置信区间，基本同样的工作流 
#也适用于基于重排的假设检验(参见图 4.18)，区别在于:
# 此时，多了一步用 hypothesize()设定原假设;
# 重复生成数据的方法不是 Bootstrap 而是 permute
# 用 infer 包实现重排假设检验的一般流程
#t检验
#t 检验是针对连续变量的参数检验，可用来检验“单样本均值与已知均值(单样本 t 检 验)、
#两独立样本均值(独立样本 t 检验)、配对设计资料的均值(配对样本 t 检验)”是否存在差异。
#t 检验适用于小样本量(比如样本数小于 60，大样本数据可以用 U 检验)，并且要求数据
#满足正态性和方差齐性(方差相等)。若不满足可尝试变换数据，或改用 Wilcoxon 符号秩/秩和检验。
#以检验电影中的爱情片与动作片的评分差异为例，数据集 movies_sample(来自 Chester Ismay 2018)
#是从 IMDb 随机抽样的 68 部电影(动作片或爱情片)，先做分组汇总探索，代码如 下所示:
load("data/movies_sample.rda")
movies_sample

movies_sample %>%
  group_by(genre) %>%
  summarise(n = n(), avg_rat = mean(rating), sd_rat = sd(rating))

#对于该样本，爱情片的平均评分为 6.32，动作片的平均评分为 5.28，二者之差为 1.04，
#这是真实差异的点估计。我们想知道，该差异能否用来推断总体(所有电影)情况，还是只是
#随机抽样的偶然因素造成的差异。
#要回答该问题，先构造假设检验:
#H0 : r  a  0 H1 : r  a  0
#在原假设 H0 下，即假设爱情片与动作片的平均评分没有差别，用重排法生成 1000 个原样
#本的重抽样数据。重排法是不重复抽样，原数据是 68 个样本，每个重抽样数据仍是不重复
#的 68 个样本，假设(在原假设下)爱情片与动作片的平均评分没有差别，那就将 genre 列随
#机重排(shuffled)，让每个电影评分随机地对应这些爱情片或动作片。 然后，对每个重排样
#本分别计算检验统计量，这里是均值差 ˆr  ˆa 。这 1000 个统计量值就是在 H0 (随机
#抽样的偶然因素)下，产生的均值差异的分布，也称为零分布。那么，这 1000个随机的统计
#量(均值差)中，有多少会比点估计值 1.04 更大呢?其占比不就是假设检验的 P 值吗?即在 
#H0 假设下，有多大的概率会出现当前观测结果。
#若该 P 值小于置信水平 0.05，则表明由随机抽样的偶然因素造成这样大的均值差异(1.04) 
#是很罕见的，因此有理由拒绝相应的原假设。
#下面给出基于 infer 包的实现。用参数 null 设定零假设，可选“point”(单样本)和 
#“independence”(两样本);用重排法生成 1000 个模拟样本;用参数 stat 指定要计算的检验
#统计量，参数 order 用设定均值差的参数顺序: 
library(infer)

movies_sample %>%
  null_distribution = movies_sample %>%
  specify(formula = rating ~ genre) %>% 
  # 响应变量~解释变量 
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "diff in means", order = c("Romance", "Action"))

null_distribution

#可视化零分布数据并标记点估计竖线及 P 值对应区域，代码如下: 
visualize(null_distribution, bins = 15) +
  shade_p_value(obs_stat = tibble(stat = 1.047), direction = "both")

#获取 P 值的代码如下:
null_distribution %>% # 获取P值
  get_p_value(obs_stat = tibble(stat = 1.047), direction = "both")

p_value

#可视化重排假设检验 P 值
#由以上结果可知，P 值 = 0.006 < 0.05，故拒绝原假设，接受备择假设，即爱情片与动作片
#的平均评分是有统计学意义上的差异的
#这里只是阐述基于重排的假设检验的基本用法，更多案例可参阅 infer 包 Vignettes。 



#4.4 回归分析
#回归分析(Regression Analysis)是统计学的核心算法，是计量模型和机器学习的最基本算法
#回归分析是确定两个或两个以上变量间相互依赖的定量关系的一种统计分析方法，具体是
#通过多组自变量和因变量的样本数据，拟合出最佳的函数关系。如果该关系是线性函数关系， 
#就是线性回归。
#计量模型和机器学习中的各种回归算法都可以看作对线性回归的扩展，分类算法也可以看
#作一种特殊的回归。
#回归分析常用于:
#探索现象/结果的影响因素主要有哪些; 
#影响因素对现象/结果是怎样影响的;
#预测未来的现象/结果
#先总体解释一下对数据进行回归建模，任何对数据进行的回归建模，都可以抽象成如下表示
#设 y 为因变量数据，x 为自变量数据(可以是多维)，设二者之间的真实(精确)关系为: y  f (x)
#该精确关系是不可能得到的，所谓回归建模只是试图去找到一种近似的关系来代替它:
# f(x) f(x)
# 二者之差就是模型的残差:
# 我们总是希望把 y 与 x 的关系都留在模型部分: f (x) ，让残差部分不再含有这种关系，最好
# ˆ
#   f(x) f(x)
# ˆ
# 只是白噪声(即完全是随机误差，均值为 0 ，标准差相对于数据本身也不太大，且服从正态分布):
#     N(0,2) 
# 如此便说明建模成功;否则，就是模型尚未提取出充分的模型关系(欠拟合)。
# ˆ
# 关于构建的模型关系 f (x) ，可以是简单的线性关系(线性回归)，也可以是复杂的“黑箱”
# 模型(神经网络、支持向量机等)。尽管无法得到精确的表达式，但该模型仍可以用于预测。 
# 另外，回归建模的一个基本原则是在没有显著差异的情况下，优先选择更简单的模型。简
# 单模型已足够充分建模，非要用更复杂的模型则会适得其反(过拟合)，降低模型的泛化(预测) 能力

#2.多元线性回归 


#4.4.2 回归诊断 
#线性回归模型的成功建模，依赖于如下假设。
#(1)线性模型假设: y  Xβ   
#(2)随机抽样假设:每个样本被抽到的概率相同且同分布
#(3)无完全共线性假设: X 满秩
#(4)严格外生性假设: E( | X )  0 
#(5)球形扰动项假设:Var( | X )   2 In 
#(6)正态性假设:|X N(0,2In)。
#其中，前三个假设是基础假设，严格外生性假设和球形扰动项假设分别保证了估计量的无偏
#性和有效性，最后一个正态性假设是为了进行统计推断做的额外假设。
#当前四个假设成立时，估计量无偏。
#当前五个假设成立时，估计量有效，是最优线性无偏估计量。 
#当所有假设都成立时，估计量是最优估计量。
#线性回归要求残差满足正态性，即  y Xβ  N(0,2)，则y  N(Xβ,2)。这说明线性回
#归通常要求因变量 y 近似服从正态分布。若 y 数据不满足正态性要求，可以考虑对 y 做变换，
#或直接考虑广义线性模型。
#线性回归模型建模是否成功，可不可以用于预测，还需要做模型检验。
#1.拟合优度检验
#计算 R2(也称为可决系数)反映了自变量所能解释的方差占总方差的百分比。分别将总平
#方和、回归平方和、残差平方和记为:
#SST 则R2定义为:
#n 2 n 2 n 2 (yi y) , SSR (yˆi y) , SSE (yi yˆi)
#i1 i1 i1
#R2 SSR1SSE SST SST
#均方根误差计算公式如下:
#adj n  p 1
#4.4 回归分析 
#R2值越大，说明模型拟合效果越好。对于数值变量的线性回归，通常可以认为当R2 0.9 时，
#所得到的回归直线拟合得很好;而当R2 0.5时，所得到的回归直线很难说明变量之间的依赖
#关系。
#R2 未考虑自由度问题，为避免增加自变量数量而高估 R2 ，选择调整的 R2 是更合理的: 
#R2 1 n1 (1R2)其中， n 为样本数， p 为自变量个数。 2.均方误差与均方根误差
#均方误差计算公式如下:
#1n 2 M S E  n ( y i  yˆ i )
#i1
#1n 2 RMSE n (yiyˆi)
#i1 均方根误差刻画的是预测值与真实值平均偏离多少，是所有回归模型(包括机器学习中的
#回归算法)最常用的性能评估指标。
#3.残差检验
#前文谈到回归建模成功与否的关键标志是残差是否为白噪声。
#残差分类图如图 4.21 所示，其中只有图(a)说明模型是成功的，把模型部分都提取出来了;
#图(e)和图(f)属于模型本身有问题，没有把模型部分提取完全;图(b)说明数据有异常点， 
#应处理掉异常点重新建模;图(c)中残差随 x 的增大而增大;图(d)中残差随 x 的增大而先增
#后减，两者都属于异方差。此时应该考虑在回归之前对数据 y 或 x 进行变换，实现方差稳
#定后再建模。原则上，当残差方差变化不太快时采用开根号变换 y ;当残差方差变化较快时取对数变
# 换 ln y;当残差方差变化很快时取逆变换 1/y;还有其他变换，如著名的 Box-Cox 变换或
# Yeo-Johnson 变换(可应付负值)，可将非正态分布数据变换为正态分布。 因此，用残差检
#验模型是否成功，就是对残差做正态性检验。也可以进一步考察学生化残
# 差(可回避标准化残差的方差齐性假设)是否服从标准正态分布。



#4.4.3 多元线性回归实例
#1.准备数据与简单探索
#现有关于企鹅的数据集 penguins，该数据集包含 333 个样本，是有关企鹅的特征信息， 
#包括种类、岛屿、嘴长、嘴宽、鳍长、性别。我们希望确定企鹅体重与这些特征的关系
penguins = read_csv("data/penguins.csv") %>%
  mutate(species = factor(species))
penguins

#先探索因变量 body_mass(体重)的分布，代码如下: 
ggplot(penguins, aes(body_mass)) +
  geom_histogram(bins = 20, fill = "steelblue", color = "black")

# 若因变量是右偏分布，可以尝试做对数变换将分布形成转复成近似正态分布。这里不做变换，
# 以 body_mass 作为因变量

#2.构建多元线性回归模型
#用 lm()函数拟合多元线性回归模型，其基本格式为:
  lm(formula, data, ...)
#其中，formula 为要拟合的回归模型的形式，例如 y ~ x1 + x2 对应模型 y01x12x2，
#默认包含截距项，若不想包含截距项可使用y ~ x1 + x2 -1。
#formula 设定模型公式遵从 Wilkinson 表示规则，更多常用写法如下所示: y ~ .:包含所有自变量的主效应。
#x1:x2:交互效应，即 x1x2 项
#x1*x2:包含全部主效应和交互效应，是对 x1 + x2 + x1:x2 的简写
#I():打包式子作为整体
#y ~ poly(x, 2, raw = TRUE):一元二次多项式回归，同y ~ x + I(x^2)
#y ~ polym(x1, x2, degree = 2, raw = TRUE):二元二次多项式回归
#log(y)~x:对 y 做对数变换
#返回值列表包含回归系数、统计量、拟合值、残差等，可以用 summary()查看汇总模型结果，
#或者用 broom 包提供的 tidy()、glance()和 augment()将模型结果变成整洁数据框。
#先把自变量都用上，构建初始多元线性回归模型(该模型往往不是成功的模型，此外省略结果):
#mdl0 = lm(body_mass ~ ., penguins)

#3.共线性诊断与逐步回归
#用 car::vif()1诊断回归模型的多重共线性，代码如下:
car::vif(mdl0)
#从以上结果可知，只有分类变量 species 的 VIF 值较大，其余均小于 10，这说明该模型
#不存在共线性。处理该共线性，可以剔除相对不那么重要的变量，或者用 step()做逐步回归，
#step()可 以剔除不显著的自变量，顺便剔除共线性的自变量。逐步回归是以 AIC 值(越小越好)
#作为加入和剔除变量的判别条件，参数 direction 可 设置逐步选择的方法:“both”“backward”(逐步剔除)、
#“forward”(逐步加入)。Akaike 信息准则(AIC 值)常用来比较不同回归模型的拟合效果，优点
#是既考虑了模型的 拟合效果又对模型参数过多这种情形施加一定惩罚，其定义为:
#其中， p 为回归模型中自变量的个数，L 为回归模板的对数似然函数
mdl1 = step(mdl0, direction = "backward",trace = 0) # 避免输出中间过程 
summary(mdl1)
#用 mctest::imcdiag()诊断回归模型的多重共线性更全面，除了计算 VIF 值外，
#还计算其他诊断指标值
#结果给出了回归系数的标准误、显著性、回归模型的标准误等，基于理论的回归系数的置
#信区间，可用 confint()来提取:
confint(mdl1)
#该模型基本上是成功的模型，回归系数都是显著的，模型的调整 R2 为 0.873。要计算模型
#的均方根误差:
library(modelr)
rmse(mdl1, penguins)

#4.关于回归模型中的分类变量
#分类变量的取值是有限的类别值，如性别:男、女。分类变量是不能直接用到回归模型中 的，
#即使用 1 表示男，用 0 表示女，这个 1 和 0 仍然只能是起类别区分的作用。如果不加处理， 
#让它们被当作数值 1 和 0 使用了，那么整个模型的逻辑和结果都是不正确的!
#因此，分类变量要想正确地用到回归模型，必须经过特殊处理，即处理成虚拟变量。
#R 中 的分类变量只要是因子型或字符型，当加入回归模型时，不需要做任何额外操作就能
#自动处理成虚拟变量用进模型。但是为了让读者理解分类变量如何用于回归模型，以及包含
#分类变量的 回归模型结果如何解读，下面拆解开来讲清楚。
#以本例的企鹅种类为例，species 列是分类变量，查看其各类别及频数，代码如下:
table(penguins$species)

#由以上结果可见，species 包含 3 个类别:“Adelie”“Gentoo”“Chinstrap”。
#虚拟变量是一种二值变量(0-1)，只能表示是否。二分类或多分类变量，可以这样转化为 虚拟变量，
#即转化为多个二值变量，即:
# species 是否为 Adelie，species 是否为 Gentoo，species 是否为 Chinstrap 比如第1个样本，
# 其species = Adelie，要用上述3个二值变量表示的话，分别为1, 0, 0
# 每个样本都做这样的处理，这就是分类变量转化为虚拟变量，可用 
# modelr::model_matrix()函数实现，其参数 data 为数据，formula 为模型公式
# 若给 formula 参数提供用于 lm()的模型公式，则返回把分类变量处理成虚拟变量之后
# 的自变量数据，这也是真正用于回归模型的自变量数据;这里只想看 species 变成虚拟变量的效果:
model_matrix(penguins, ~ species - 1)

#也就是说，不使用原 species 列，而是将新的虚拟变量列用到回归模型。但是要注意一个 
#问题:这 3 个虚拟变量列是线性相关的，每一列都能用其余 2 列线性表示(1 减去其余 2 列)， 
#换句话说有一列数据是冗余的，线性回归也是坚决不允许存在这样的线性相关列的。
#因此，我们需要去掉任意一列，再进行线性回归建模。去掉哪一列都可以，去掉的是哪一列，
#做回归建模就相当于以谁为参照列。比如去掉 species 是否为 Adelie 列，就相当于将
#“Adelie”组作为参照组，用另外 2 组“Gentoo”“Chinstrap”与参照组做比较。 
#去掉冗余列，再增加截距列(一列 1)，才是将 species 列真正用于回归模型且转化为虚
#拟变量后的数据:
model_matrix(penguins, ~ species)

#冗余列默认要去掉第一水平，若想去掉另一水平(该组作为参照组)，可以借助 relevel() 修改第一水平，
#再将其处理成虚拟变量:
penguins$species = relevel(penguins$species, ref = "Gentoo")
#根据前文逐步回归得到的 mdl1 的回归系数估计，可以写出拟合的回归方程: 
#连续变量的回归系数比较容易理解，比如 bill_length 的系数 18.204，表示企鹅嘴长每
#增加 1 个单位(毫米)，体重将增加 18.204 个单位(克)。
#对于分类变量回归系数的解释，先看原二分类变量 sex，变成虚拟变量去掉冗余列后只剩
#一列 sexmale(是否为雄性，1 代表是，0 代表否)，代入模型来看:
#若性别不是雄性，则 SexMale = 0
#若性别是雄性，则 SexMale = 1
#即雌性则加 0，雄性则加 389.892, 这就相当于以雌性为参照组，雄性的体重平均比雌性
#重 389.982 克，这就是该回归系数的意义。
#再看原多分类变量 species(3 分类)，变成虚拟变量需去掉冗余列 speciesAdelie 后 
#剩下 2 列。若种类是 Adelie，则这两列均为 0, 即回归模型不包含这两项，此时是参照组;
#若种类是任一非参照组，比如Gentoo，则speciesGentoo = 1,此时回归模型多了一项:
#这就相当于以 Adelie 为参照组，Gentoo 组相对于参照组 Adelie 平均体重要重
#1014.627 克。总之，分类变量用于回归模型，所起的作用就是在分组之间做比较
#这实际上也等效于分别对各分组建立线性回归模型，再做比较
#切记:分类变量用于建模时，始终是起分类的作用，绝对不能因为表示为数值形式，
#就直接当作数值使用


#5.模型改进
#多元线性回归模型的改进，通常的方法是特征工程，即构建新特征。 自变量又称为特征，
#利用原有自变量构造新的自变量，就是特征工程。特征工程是数据挖掘和机器学习中的关
#键步骤。 我们知道，泰勒公式是用多项式曲线逼近非线性曲线，随着展开次数的增加，逼
#近效果往往会越来越好。多元线性回归相当于用一次多项式去逼近真实的函数关系，如果提
#高到二次，即把所有 二次项包括交互项都加入模型1，即x2,x2,x x ,拟合效果大概率会
#有提升。但是，这会带来另外的问题:新加入的项可能会有不显著或产生共线性。解决办法，
#就是用逐步回归进行变量筛选。 这些二次项的构建就是构建特征。另外，常用的构建特征
#方法是对特征做各种变换2，其中一 种是连续特征离散化，比如年龄相差 1 岁的影响不
#一定显著，但较大的年龄段(比如从青年到中(比如从青年到中年到老年)的差异，很可能会
#显著。另外，构建自然样条特征也是引入非线性关系的更好的做法。
#代码如下:
  mdl2 = lm(body_mass ~ species + sex * island + bill_length + I(bill_length^2)
            + bill_depth + I(bill_depth^2) + flipper_length
            + I(flipper_length^2), penguins) %>%
  step(direction = "backward", trace = 0)
summary(mdl2)

#1 关于交互项 x1:x2 的解释:x1 对 y 的影响受 x2 的调节，反之亦同，其回归系数相当于
#  y 对 x1 和 x2 的二阶偏导。
#2 在实际的特征工程、建模、甚至是解决任何问题的过程中，从常理去思考非常重要，这也
#  是思路的主要来源!

#可见，二次项 flipper_length^2 项和交互项 sexmale:islandDream 都非常显著，
#模型的调整 R2 比 mdl1 稍有提高(0.0028)。
#这说明 mdl2 相比 mdl1 有所改进，但同时也增加了模型的复杂度(多了 4 项)。那么，接
#受哪个模型更好呢?基本原则是在模型没有显著差异的情况下，优先选择更简单的模型。 
#可用似然比检验 lmtest::lrtest()或方差分析 anova()比较两个模型有无显著差异:
anova(mdl1, mdl2)

#检验P值= 0.025小于0.05，说明两个模型有显著差异，应该选择mdl2.


#6.回归诊断
#残差检验
#前文讲到理想的模型(标准化)残差应服从“0 均值小方差”(标准)正态分布，对于残差，
#通常是绘制(标准化)残差图、残差 QQ 图、残差直方图，或者对(标准化)残差的正态性、
#独立性、异方差性做统计检验。
#强影响分析 对参数估计或预测值有异常影响的数据，称为强影响数据。回归模型应当具
#有一定的稳定性，若个别样本数据对估计有异常大的影响，剔除这部分数据后，若得到与
#原来差异很大的回 归方程，就有理由怀疑原回归方程是否真正描述了变量间的客观存在的关系。
#这些强影响样本是异常值，应当识别出来并剔除之后(见第 5 章)，再重新拟合回归模型
#残差图可以用 augment()+ ggplot()自行绘制，更简单的做法是直接用 ggfortify::autoplot()
#绘制回归诊断图，包括残差图、残差 QQ 图、标准化残差图、强影 响图等，还能同时标记
#强影响样本。具体代码如下。
library(ggfortify)
autoplot(mdl2, which = c(1:3,6)) # 6个图形可选
#比图形诊断更可靠的是对残差做一些统计检验，代码如下:
I(flipper_length^2) + sex:island
shapiro.test(mdl2$residuals)

library(lmtest)
dwtest(mdl2)
bptest(mdl2) # 残差异方差检验

#可见，mdl2 能通过残差正态性、独立性检验。剔除强影响样本，重新拟合回归模型略。 

#7.回归模型预测
#通过检验的回归模型，提供新的自变量数据框，用 predict()函数就可以预测因变量值。
newdat = slice_sample(penguins[,-6], n = 5)
predict(mdl2, newdat, interval = "confidence")

#4.4.4 梯度下降法
#用正规方程法求解多元线性回归，简单且容易实现，但也有其缺点:
#若 X T X 不可逆，则正规方程法失效;
#若样本量非常大(样本数 10000)，矩阵求逆会非常慢。
#再来介绍一种方法，同时也是广泛用于机器学习算法中的做法— 梯度下降法，其核心思想
#是迭代地调整参数，使损失函数达到最小值。
#梯度下降法就好比在浓雾笼罩的山顶向山下走，每次只能看到前方一步远，那么就要环顾
#每个方向考虑向哪个方向迈一步下降的高度最多，那就往哪个方向迈一步。重复该过程，
#逐步到达更低的位置(不一定是最低 点)，如图 4.24 所示。
#根据数学知识，下降最快的方向就是负梯度方向!
#具体到线性回归问题，就是计算损失函 数 J ( ) 关于参数向量  的局部梯度，同时
#它沿着梯度下降的方向进行下一次迭代。当梯度值为零的时候，就达到了损失函数的最小值。 
#开始需要选定一个随机的  (初始值)，然后逐渐去改进它，每一次变化一小步，每一步都
#试着降低损失函数 J ( ) ，直到算法收敛到一个极小值。 该极小值不一定是全局最小值，
#若损失函数是凸函数(线性回归损失函数是凸函数)，则极小值就是唯一的全局最小值。 
#梯度下降法的重要参数是每一步的步长，又称学习率。一个好的策略是，开始的学习率大一
#些以更快速趋于收敛，之后让学习率慢慢减小，最后阶段要让学习率足够小以稳定地到达收敛点。
#注意:梯度下降法对自变量取值的量级是敏感的，若所有自变量的数量级基本相当，则能更快
#地收敛到最 小值。在用梯度下降法训练模型时，有必要对数据做归一化(放缩)，以加速训练。
#线性回归模型的损失函数为(在下式中，除以 2 用于抵消求偏导的系数): 
# 1n(i) 2
# J()2n (x yi) i1
#在梯度下降法中，需要计算每一个 βj(维度)下损失函数的梯度。换句话说，需要计算当 βj 
#变化一点点时，损失函数改变了多少，这就是偏导数:
#  1n (i)
#  J(β)  n (x   yi )xij, j 1,,m
# j


#i1 将上式改为向量化表示，可得到损失函数的梯度向量:
#J(β)  J(β),,  J(β)1XT(Xβy)   n
#1m 注意，梯度下降法中每一步梯度向量的计算，都是基于整个训练集(每一次训练过程都使
#用所有的训练数据)，故称为批量梯度下降。因此，在大数据集上，训练速度也会变得很慢1， 
#但其复杂度是 O(n)，比正规方程法 O(n3)快得多。
#梯度向量有了，只需要每步以学习率 η 调整参数即可: βnext βJ(β)
#接下来定义函数实现梯度下降法求解线性回归2。
gd = function(X, y, init, eta = 1e-3, err = 1e-3, maxit = 1000, adapt = FALSE) {
  # 初始化
  X = cbind(Intercept = 1, X)
  beta = init
  names(beta) = colnames(X)
  loss = crossprod(X %*% beta - y) tol = 1
  iter = 1
  # 迭代
  while(tol > err && iter < maxit) {
    LP = X %*% beta
    grad = t(X) %*% (LP - y)
    betaC = beta - eta * grad
    tol  = max(abs(betaC - beta))
    beta = betaC
    loss = append(loss, crossprod(LP - y))
    iter = iter + 1
    if(adapt)
      eta = ifelse(loss[iter] < loss[iter-1], eta * 1.2, eta * 0.8)
  }
  list(beta = beta, loss = loss, iter = iter, fitted = LP,
       RMSE = sqrt(crossprod(LP - y) / (nrow(X) - ncol(X))))
}

#用随机生成数据的二元线性回归来测试函数，代码如下。
n = 1000
set.seed(123)
x1 = rnorm(n)
x2 = rnorm(n)
y = 1 + 0.6*x1 - 0.2*x2 + rnorm(n)
X = cbind(x1, x2)
gd_rlt = gd(X, y, rep(0,3), err = 1e-8, eta = 1e-4, adapt = TRUE)
rbind(gd = round(gd_rlt$beta[, 1], 5),
      lm = coef(lm(y ~ x1 + x2))) # 与lm结果对比
# 迭代次数 plot(gd_rlt$loss, xlab = "迭代次数", ylab = "损失")


#可见，算法收敛速度非常快，迭代 14 步之后损失函数基本就不再减小
#线性回归是回归家族的基本模型，从不同角度进行扩展可以衍生出几十种回归模型。
#简单介绍线性回归的一种自然推广— 广义线性模型。广义线性模型用 glm()函数实现，
#通过 family参数设置分布名，以决定选用的模型。 线性回归要求因变量是服从正态分布的
#连续型数据。但实际上，因变量数据可能会是类别型、计数型等。 要让线性回归也适用于
#因变量非正态连续情形，就需要推广到广义线性模型。Logistic 回归、softmax 回归、泊松
#回归、Probit 回归、二项回归、负二项回归、最大熵模型等都是广义线性模 型的特例
#广义线性模型相当于复合函数。先做线性回归，后接一个变换: wT X  b  u  正态分布
#经过变换后得到非正态分布的因变量数据
#我们一般更习惯反过来写:即对因变量 y 做一个变换，就是正态分布，从而就可以做线性回归:
# y wT X b 其中， 称为连接函数

# 注意:因变量数据只要服从指数族分布，如正态分布、伯努利分布、泊松分布、指数分布、
# Gamma 分布、 卡方分布、Beta 分布、狄里克雷分布、Categorical 分布、Wishart 分布、
# 逆 Wishart 分布等，就可以使用对 应的广义线性模型。
# 泊松回归和负二项回归都是针对因变量是计数数据的情况，区别是泊松回归一般用于个体 
# 之间独立的情形;负二项回归则可用于个体之间不独立的情形。
# 拓展学习
# 读者如果想进一步了解统计学理论及 R 实现，建议大家去阅读冯国双编写的《白话统计》，
# 贾俊平编写 的《统计学(第 7 版)》，Chester 等人编写的 Statistical Inference via 
# Data Science A ModernDive into R and the Tidyverse，Mine Ç R 等人编写的 
# Introduction to Modern Statistics，以及参考 rstatix 包、infer 包、maxLik 包、
# lmtest 包文档及相关资源